{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "460f2d4d4fe04bf3a01eec4061bf2f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cc67988d829f42c2bba23d08058425a4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3183e8e31396424a8a3823d19dce62e7",
              "IPY_MODEL_b9e1d0ea7a9544b9a8c153f6a5b41947"
            ]
          }
        },
        "cc67988d829f42c2bba23d08058425a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3183e8e31396424a8a3823d19dce62e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dd2ba91c28934059b31599706df31bb8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5069051,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5069051,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0cf2fef723a47a09a4b7d51c182e825"
          }
        },
        "b9e1d0ea7a9544b9a8c153f6a5b41947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_46ce73f12ebb413ba6daa825df82297a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 7.13MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efc41d45404e412aa31d1d19f31ed277"
          }
        },
        "dd2ba91c28934059b31599706df31bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0cf2fef723a47a09a4b7d51c182e825": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46ce73f12ebb413ba6daa825df82297a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efc41d45404e412aa31d1d19f31ed277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d3d0849fe6b4d26bf94c62400c1a0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cae6043a4bdb49608cfcf08c3e8c6a77",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_260f205ebbfd4f4f9af487a0c30a0e27",
              "IPY_MODEL_26a1a8e1143a41dc986b7145b535deaa"
            ]
          }
        },
        "cae6043a4bdb49608cfcf08c3e8c6a77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "260f205ebbfd4f4f9af487a0c30a0e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_78c3d4ddfaeb4f37b8749960761a9231",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 512,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 512,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd28fdc1cb1143da8b7710fb53fa9408"
          }
        },
        "26a1a8e1143a41dc986b7145b535deaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_57da920a2fda4318945da0b88e0cc16e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 512/512 [00:26&lt;00:00, 19.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d52e05f87844603b8bf0d19f64e4184"
          }
        },
        "78c3d4ddfaeb4f37b8749960761a9231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd28fdc1cb1143da8b7710fb53fa9408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57da920a2fda4318945da0b88e0cc16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d52e05f87844603b8bf0d19f64e4184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93bdb902ecf04788bfd652078f2a624e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_156a87f596084399ac053934e1aa79ec",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2bfc54a55c4b4684b2dba24600420949",
              "IPY_MODEL_f0e86ae29ca34d9ba0cf14623d4388ce"
            ]
          }
        },
        "156a87f596084399ac053934e1aa79ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bfc54a55c4b4684b2dba24600420949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3a16d4d223aa4bf695755225fc83d897",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1115590446,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1115590446,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb6a3b383ae34c0c853a23396de4cfb1"
          }
        },
        "f0e86ae29ca34d9ba0cf14623d4388ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_61c1f2c5dbb943cf895866a49747bfce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.12G/1.12G [00:25&lt;00:00, 42.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eefdab8b91894c62b50eccb89ab8e8cd"
          }
        },
        "3a16d4d223aa4bf695755225fc83d897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb6a3b383ae34c0c853a23396de4cfb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61c1f2c5dbb943cf895866a49747bfce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eefdab8b91894c62b50eccb89ab8e8cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mrZuRqqEDWA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui-6mqCFOsi8"
      },
      "source": [
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import os\r\n",
        "from tqdm import tqdm, trange\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5IXVFn5_3hq",
        "outputId": "a31108e1-9929-4945-d8a2-2c4f903e9667"
      },
      "source": [
        "import torch\r\n",
        "if torch.cuda.is_available():  \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "    \r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyVokmAmBlUn"
      },
      "source": [
        "# Utilities\r\n",
        "from time import time\r\n",
        "from PIL import Image\r\n",
        "from zipfile import ZipFile\r\n",
        "import os, sys, itertools, re\r\n",
        "import warnings, pickle, string\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4VrAis5BbFR",
        "outputId": "63bac3c1-d4ec-4ca3-82d1-ee633beaccf8"
      },
      "source": [
        "# Block which runs on both Google Colab and Local PC without any modification\r\n",
        "if 'google.colab' in sys.modules:    \r\n",
        "    project_path = \"/content/drive/My Drive/NER/\"\r\n",
        "    # Google Colab lib\r\n",
        "    from google.colab import drive\r\n",
        "    # Mount the drive\r\n",
        "    drive.mount('/content/drive/', force_remount=True)\r\n",
        "    sys.path.append(project_path)\r\n",
        "    %cd $project_path\r\n",
        "\r\n",
        "# Let's look at the sys path\r\n",
        "print('Current working directory', os.getcwd())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/NER\n",
            "Current working directory /content/drive/My Drive/NER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc9Asq37k4qD",
        "outputId": "4bb2417d-bb47-4249-85e1-aba2d9dfeb50"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\r\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\r\n",
        "# For example, here's several helpful packages to load in \r\n",
        "\r\n",
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "\r\n",
        "# Input data files are available in the \"../input/\" directory.\r\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\r\n",
        "\r\n",
        "import os\r\n",
        "print(os.listdir(\"../NER\"))\r\n",
        "\r\n",
        "# Any results you write to the current directory are saved as output.\r\n",
        "from tqdm import tqdm, trange\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ner.csv', 'ner_dataset.csv', 'xlmner.pt', 'restauranttrain.bio', 'restauranttest.bio', 'cache_dir', 'outputs', 'runs', 'modelner', 'modelxlmner.pt', 'drive', 'xlm_ner1.pt', 'name.pt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXoUhlYgCErR"
      },
      "source": [
        "\r\n",
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "bEC2SSyE_JIO"
      },
      "source": [
        "input_data = pd.read_csv(\"/content/drive/MyDrive/NER/ner_dataset.csv\", encoding=\"latin1\")\n",
        "#input_data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "fa91210f9bc73bfa267e2420780f492d83be7af1",
        "scrolled": true,
        "id": "9paaElun_JIQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "a19ba17d-6c3a-451c-b943-967b96c7664e"
      },
      "source": [
        "input_data = input_data.fillna(method=\"ffill\")\n",
        "input_data.tail(10)\n",
        "input_data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048570</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>they</td>\n",
              "      <td>PRP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048571</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>responded</td>\n",
              "      <td>VBD</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048572</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048573</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048574</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>attack</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1048575 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              Sentence #           Word  POS Tag\n",
              "0            Sentence: 1      Thousands  NNS   O\n",
              "1            Sentence: 1             of   IN   O\n",
              "2            Sentence: 1  demonstrators  NNS   O\n",
              "3            Sentence: 1           have  VBP   O\n",
              "4            Sentence: 1        marched  VBN   O\n",
              "...                  ...            ...  ...  ..\n",
              "1048570  Sentence: 47959           they  PRP   O\n",
              "1048571  Sentence: 47959      responded  VBD   O\n",
              "1048572  Sentence: 47959             to   TO   O\n",
              "1048573  Sentence: 47959            the   DT   O\n",
              "1048574  Sentence: 47959         attack   NN   O\n",
              "\n",
              "[1048575 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ef6bcf215604b4247864be63c03e9032f6f558e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqaYaker_JIS",
        "outputId": "e9e881eb-5d16-4146-b484-20a092d447b0"
      },
      "source": [
        "words_list = list(set(input_data[\"Word\"].values))\n",
        "words_list[:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sneaking',\n",
              " 'hurricane-',\n",
              " 'visit',\n",
              " 'Beit',\n",
              " 'pollsters',\n",
              " 'Neskovic',\n",
              " 'Re-elected',\n",
              " 'Witnesses',\n",
              " 'Baghaichhari',\n",
              " 'dubious']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ae74a5b70b9aa9dece4959b80d4bff4a6071e812",
        "id": "mFRXpn4j_JIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8ace5c-cd76-413b-f918-6703e4e7dff1"
      },
      "source": [
        "number_words = len(words_list); number_words # number of unique words in the corpus"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35178"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "87152fde7e3ea0c671779d932fe1044124e6109c",
        "id": "TS9uXxe7_JIV"
      },
      "source": [
        "class RetrieveSentance(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        function = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(function)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def retrieve(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "305b004fd4bd1ef52a4d8bfef554ab326361d946",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50GOQec9_JIW",
        "outputId": "6e00a87f-c18b-453a-d75c-c9aa3fa91f28"
      },
      "source": [
        "Sentences = RetrieveSentance(input_data) \n",
        "Sentences"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RetrieveSentance at 0x7f8adfb1c978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e89d28e02d1bacbce7359347ae94600bb987ee3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K-Sd4N2l_JIX",
        "outputId": "154936f9-04c9-4ea3-be29-f3050aa105ff"
      },
      "source": [
        "Sentences_list = [\" \".join([s[0] for s in sent]) for sent in Sentences.sentences]\n",
        "Sentences_list[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8c91c430ec4d252b913264e8422301487f60c512",
        "id": "2Gl0GvNg_JIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d372613d-b7ec-4ab6-df37-ccb5065dc1d6"
      },
      "source": [
        "len(Sentences_list) #number of sentences "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47959"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a5d6c76cf541b7c95e0491b4bdae793f0b6aedc8",
        "id": "iCK_IIwM_JIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328cd275-7250-410e-e231-d1b1b6477461"
      },
      "source": [
        "labels = [[s[2] for s in sent] for sent in Sentences.sentences]\n",
        "print(labels[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3ecde07cca4872deb8c0173d2888d0864f616308",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nms6OnD6_JIb",
        "outputId": "640a27a8-feb7-4841-ecc7-1ca731bfa0ab"
      },
      "source": [
        "labels [0] # list of lists of dimension (sentences,labels)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-geo',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-geo',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-gpe',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "fea40191f48b48c21e42a6871552919a43a33d22",
        "id": "07OSo1K-_JIc"
      },
      "source": [
        "tags2vals = list(set(input_data[\"Tag\"].values))\n",
        "tag2idx = {t: i for i, t in enumerate(tags2vals)}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0e13cda88f4e0b49d75ccfe9f62f6d90e35c8a5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNqgjAOz_JId",
        "outputId": "67678b4d-7e9e-4b38-a251-0be97f9121c7"
      },
      "source": [
        "tags2vals # 17 kinds of tags "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-art',\n",
              " 'I-nat',\n",
              " 'I-geo',\n",
              " 'I-gpe',\n",
              " 'B-nat',\n",
              " 'B-org',\n",
              " 'B-eve',\n",
              " 'I-per',\n",
              " 'I-eve',\n",
              " 'B-gpe',\n",
              " 'B-per',\n",
              " 'B-tim',\n",
              " 'I-art',\n",
              " 'I-tim',\n",
              " 'I-org',\n",
              " 'B-geo',\n",
              " 'O']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1b2768a62ede82a25182cba9d73e65a47dd6f1e2",
        "id": "mziRrnzd_JIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542f4271-d487-4980-cea8-d5324f05761c"
      },
      "source": [
        "tag2idx # indexing the tag "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-art': 0,\n",
              " 'B-eve': 6,\n",
              " 'B-geo': 15,\n",
              " 'B-gpe': 9,\n",
              " 'B-nat': 4,\n",
              " 'B-org': 5,\n",
              " 'B-per': 10,\n",
              " 'B-tim': 11,\n",
              " 'I-art': 12,\n",
              " 'I-eve': 8,\n",
              " 'I-geo': 2,\n",
              " 'I-gpe': 3,\n",
              " 'I-nat': 1,\n",
              " 'I-org': 14,\n",
              " 'I-per': 7,\n",
              " 'I-tim': 13,\n",
              " 'O': 16}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuR3vOmiC_72",
        "outputId": "3379aa09-368f-46d4-d9f4-3a785923d0ec"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 24.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 54.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=a1559119bf07ece560c14e8bc6f71bf4082b6342f5ff91b81bf019d518e9d0b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2goEmbZVVQA5",
        "outputId": "314f2f4c-4d87-40c4-839b-7e4d078a7ba1"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.8MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 28.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 33.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 23.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 22.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 16.3MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 16.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 15.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 14.8MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 15.9MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 15.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 15.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 15.9MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 15.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 15.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 15.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 15.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "fb9b81a53f7ffbc6a2b2fc46aa9fec336d51a5e9",
        "id": "lRb35Ld1_JIf"
      },
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import XLMRobertaTokenizer\n",
        "import tensorflow\n",
        "import sentencepiece\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b68404b6ffc4879f374b4bd190559aeef94ddce8",
        "id": "a9hi-I62_JIh"
      },
      "source": [
        "max_seq_len = 75 # tokens\n",
        "batch_s = 32 # batch size"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9a58a0ecf1eb30dcc554a87ab819505dec6d99ef",
        "id": "CYv7z3ly_JIh"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c5dd0ef060e6d26d1ab695725bbe7a26c8c6f4c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r_3KG3EJ_JIi",
        "outputId": "176e3d28-efb7-45ce-a315-0fa890676841"
      },
      "source": [
        "torch.cuda.get_device_name(0) "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "dc6cd04bb8e9ba1d4bf398ee9fc3af97ad7d0fc3",
        "id": "aoo3GblM_JIj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "460f2d4d4fe04bf3a01eec4061bf2f8e",
            "cc67988d829f42c2bba23d08058425a4",
            "3183e8e31396424a8a3823d19dce62e7",
            "b9e1d0ea7a9544b9a8c153f6a5b41947",
            "dd2ba91c28934059b31599706df31bb8",
            "f0cf2fef723a47a09a4b7d51c182e825",
            "46ce73f12ebb413ba6daa825df82297a",
            "efc41d45404e412aa31d1d19f31ed277"
          ]
        },
        "outputId": "a8884ca4-d434-4eb4-faf2-6de542afef3c"
      },
      "source": [
        "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\" )\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "460f2d4d4fe04bf3a01eec4061bf2f8e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "963cb752037ba1421031301f54213366d0f808f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLlvAchc_JIk",
        "outputId": "73c6ed12-f729-4f3b-aef1-dfaf5aa4d7f5"
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in Sentences_list]\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁Tho', 'usan', 'ds', '▁of', '▁demonstra', 'tors', '▁have', '▁marche', 'd', '▁through', '▁London', '▁to', '▁protest', '▁the', '▁war', '▁in', '▁Iraq', '▁and', '▁demand', '▁the', '▁withdraw', 'al', '▁of', '▁British', '▁tro', 'ops', '▁from', '▁that', '▁country', '▁', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d5ee43039cd8cc6ca909f3ce15bcd49d00c17153",
        "id": "_4BxxBPN_JIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7dc738-3d23-4c74-8c97-cf67d26fe518"
      },
      "source": [
        "len(tokenized_texts)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47959"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e06d5dbd8af2785104d80a48484d6c3673db756d",
        "id": "C7AoUkPF_JIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fbe3fed-7f2e-4d43-eac2-bce607043021"
      },
      "source": [
        "print(tokenized_texts[1])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁Iran', 'ian', '▁official', 's', '▁say', '▁they', '▁expect', '▁to', '▁get', '▁access', '▁to', '▁seal', 'ed', '▁sensitive', '▁parts', '▁of', '▁the', '▁plant', '▁Wednesday', '▁', ',', '▁after', '▁an', '▁I', 'A', 'EA', '▁sur', 've', 'illa', 'nce', '▁system', '▁begin', 's', '▁function', 'ing', '▁', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "56456d4978d04fac19ffbb0d22a613a0734e4c1c",
        "id": "2kLek0aF_JIn"
      },
      "source": [
        "X = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=max_seq_len, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d42470c504501848fa88fb829f97ce162d80ef6c",
        "id": "VGFOnDsW_JIo"
      },
      "source": [
        "Y = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=max_seq_len, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "41f57a758891a4d9ed7eb444c03583f88aa786ba",
        "scrolled": false,
        "id": "VmVojgAC_JIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c01c4df-74b7-4168-ea03-055123ccde02"
      },
      "source": [
        "X.shape # (sentences, maximum sequence length)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47959, 75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9f7af71fe6b7bbe9ce804fcf1dd4472ca5366fbe",
        "id": "7quJ6u7m_JIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b64a661-9211-4d54-823b-9a9e46345b44"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47959, 75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2902f1b07bca78a0b34698ef64b1f35eabf50b2f",
        "id": "SIbqsDqx_JIq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d297744-e0b1-4b3f-b22f-89fed2709780"
      },
      "source": [
        "X"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 32238,  38504,   6468, ...,      0,      0,      0],\n",
              "       [ 18721,   3378,  51521, ...,      0,      0,      0],\n",
              "       [  1529,   9120,  78478, ...,      0,      0,      0],\n",
              "       ...,\n",
              "       [ 77168,    214,  18721, ...,      0,      0,      0],\n",
              "       [ 66016,   7068,      6, ...,      0,      0,      0],\n",
              "       [   581,  14098, 145704, ...,      0,      0,      0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "537fb21b3637534a653aec22b8be933ffb03207c",
        "id": "QNBvydPb_JIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4a087e-033a-49cc-8ee2-c255d349cd1a"
      },
      "source": [
        "Y"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16, 16, 16, ..., 16, 16, 16],\n",
              "       [ 9, 16, 16, ..., 16, 16, 16],\n",
              "       [16, 16, 11, ..., 16, 16, 16],\n",
              "       ...,\n",
              "       [16, 15, 16, ..., 16, 16, 16],\n",
              "       [16, 16, 16, ..., 16, 16, 16],\n",
              "       [16,  5, 14, ..., 16, 16, 16]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4a5204c0bd9f595f24f59c69dba950c3774e359a",
        "id": "0q1WI_Jp_JIs"
      },
      "source": [
        "attention_masks = [[float(i>0) for i in ii] for ii in X]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "13abf578090d6191fcf32e6e970ed15e4d17595a",
        "id": "_vjleDKW_JIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534651f1-003e-40e1-c27e-5f567dd7beeb"
      },
      "source": [
        "len(attention_masks) # list of lists of shape (sentences, labels )"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47959"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "aa7abbc268526511cb457a992adbd4e0a2b0c9cb",
        "id": "vWQDZ-S8_JIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531347d4-9029-41a9-97af-e66b41ceb517"
      },
      "source": [
        "attention_masks[0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "64bdffc7ba465fb18a0c46b11daba59b7f86a183",
        "id": "3hj6vaO1_JIv"
      },
      "source": [
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, \n",
        "                                                            random_state=20, test_size=0.1)\n",
        "Mask_train, Mask_valid, _, _ = train_test_split(attention_masks, X,\n",
        "                                             random_state=20, test_size=0.1)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3e0d6c5224658787d50addfd5d40bbc602914ee2",
        "id": "Kob5QCZW_JIw"
      },
      "source": [
        "X_train = torch.tensor(X_train)\n",
        "X_valid = torch.tensor(X_valid)\n",
        "Y_train = torch.tensor(Y_train)\n",
        "Y_valid = torch.tensor(Y_valid)\n",
        "Mask_train = torch.tensor(Mask_train)\n",
        "Mask_valid = torch.tensor(Mask_valid)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d786b080f7c6f705c8dcd50a7ee6d9acf893b38b",
        "id": "Lboqc54Q_JIw"
      },
      "source": [
        "data_train = TensorDataset(X_train, Mask_train, Y_train)\n",
        "data_train_sampler = RandomSampler(data_train)\n",
        "DL_train = DataLoader(data_train, sampler=data_train_sampler, batch_size=batch_s)\n",
        "\n",
        "data_valid = TensorDataset(X_valid, Mask_valid, Y_valid)\n",
        "data_valid_sampler = SequentialSampler(data_valid)\n",
        "DL_valid = DataLoader(data_valid, sampler=data_valid_sampler, batch_size=batch_s)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXcJtzU9KPWc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "b0fa6348-dacf-46ba-a4f1-9d2de3b028d3"
      },
      "source": [
        "pip install fairseq"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/da/7c7032988dade3b21ccfd5b226e50b382abfd3459129d67240bb004506ae/fairseq-0.10.1-cp36-cp36m-manylinux1_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.7.0+cu101)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.21)\n",
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/1f/7f502b9e37596164111655861370b08626f46f9e4524433c354f472765d4/hydra_core-1.0.4-py3-none-any.whl (122kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 61.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.19.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 61.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (3.3.0)\n",
            "Collecting omegaconf>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 52.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, PyYAML\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141231 sha256=f30f5c83b97b8e1249fc4732550d710131882e4aa1b21c295f0f6c904f08235f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=7d6ee3dff3c5292097376b068b05216073c9fa8a1474eddc8212e2fc8478dc56\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built antlr4-python3-runtime PyYAML\n",
            "Installing collected packages: portalocker, sacrebleu, antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 antlr4-python3-runtime-4.8 fairseq-0.10.1 hydra-core-1.0.4 omegaconf-2.0.5 portalocker-2.0.0 sacrebleu-1.4.14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p86lmQAeKpTU"
      },
      "source": [
        "from transformers import XLMRobertaTokenizer,XLMRobertaForTokenClassification\r\n",
        "import torch\r\n",
        "import sentencepiece\r\n",
        "\r\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\r\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjt8SuOnVxe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "2d3d0849fe6b4d26bf94c62400c1a0e2",
            "cae6043a4bdb49608cfcf08c3e8c6a77",
            "260f205ebbfd4f4f9af487a0c30a0e27",
            "26a1a8e1143a41dc986b7145b535deaa",
            "78c3d4ddfaeb4f37b8749960761a9231",
            "cd28fdc1cb1143da8b7710fb53fa9408",
            "57da920a2fda4318945da0b88e0cc16e",
            "1d52e05f87844603b8bf0d19f64e4184",
            "93bdb902ecf04788bfd652078f2a624e",
            "156a87f596084399ac053934e1aa79ec",
            "2bfc54a55c4b4684b2dba24600420949",
            "f0e86ae29ca34d9ba0cf14623d4388ce",
            "3a16d4d223aa4bf695755225fc83d897",
            "fb6a3b383ae34c0c853a23396de4cfb1",
            "61c1f2c5dbb943cf895866a49747bfce",
            "eefdab8b91894c62b50eccb89ab8e8cd"
          ]
        },
        "outputId": "76f77adf-ed64-4a4c-e468-7aec877e7d14"
      },
      "source": [
        "model=XLMRobertaForTokenClassification.from_pretrained(\"xlm-roberta-base\", num_labels=17)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d3d0849fe6b4d26bf94c62400c1a0e2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93bdb902ecf04788bfd652078f2a624e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebBCX9MyYgAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "300c9762-f9f7-4db6-ada5-3b7d13687ad2"
      },
      "source": [
        "print('Loading model to GPU...')\r\n",
        "\r\n",
        "# Connect to the GPU.\r\n",
        "device = torch.device('cuda')\r\n",
        "\r\n",
        "# Report what GPU we were granted.\r\n",
        "print('  GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# Copy the initial model weights to the GPU.\r\n",
        "desc = model.to(device)\r\n",
        "\r\n",
        "print('    DONE.')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model to GPU...\n",
            "  GPU: Tesla T4\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJU_4Lm5gd1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f712ee37-b99b-4855-d14e-d193b889d60d"
      },
      "source": [
        "len(tag2idx)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f709552931616bc829dba371fb172d1d9d761092",
        "id": "QVWj4lID_JIz"
      },
      "source": [
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters()) \n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qen20553FKNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862425c9-8561-43fa-c7bd-8848f5e41049"
      },
      "source": [
        "pip install seqeval"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 23.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=381bdbd18dc70f2848800e8cfb4892e3ffcca5d23ae3c89442859513c43832d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3011c3b0a523596d9541f59c5135383b6313f70f",
        "id": "yBvLi9zC_JI0"
      },
      "source": [
        "from seqeval.metrics import f1_score\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbfwLMnbWLmw"
      },
      "source": [
        "from transformers import AdamW\r\n",
        "\r\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \r\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\r\n",
        "optimizer = AdamW(model.parameters(),\r\n",
        "                  lr = 5e-6, # args.learning_rate\r\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\r\n",
        "                )"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5kMshIpWPpS"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "# Number of training epochs.\r\n",
        "epochs = 5\r\n",
        "\r\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \r\n",
        "# (Note that this is not the same as the number of training samples).\r\n",
        "total_steps = len(DL_train) * epochs\r\n",
        "\r\n",
        "# Create the learning rate scheduler.\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
        "                                            num_warmup_steps = 0,\r\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU41CBbyWYhH"
      },
      "source": [
        "import time\r\n",
        "import datetime\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))  "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1b3I_DcWbjv"
      },
      "source": [
        "def good_update_interval(total_iters, num_desired_updates):\r\n",
        "    '''\r\n",
        "    This function will try to pick an intelligent progress update interval \r\n",
        "    based on the magnitude of the total iterations.\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "      `total_iters` - The number of iterations in the for-loop.\r\n",
        "      `num_desired_updates` - How many times we want to see an update over the \r\n",
        "                              course of the for-loop.\r\n",
        "    '''\r\n",
        "    # Divide the total iterations by the desired number of updates. Most likely\r\n",
        "    # this will be some ugly number.\r\n",
        "    exact_interval = total_iters / num_desired_updates\r\n",
        "\r\n",
        "    # The `round` function has the ability to round down a number to, e.g., the\r\n",
        "    # nearest thousandth: round(exact_interval, -3)\r\n",
        "    #\r\n",
        "    # To determine the magnitude to round to, find the magnitude of the total,\r\n",
        "    # and then go one magnitude below that.\r\n",
        "\r\n",
        "    # Get the order of magnitude of the total.\r\n",
        "    order_of_mag = len(str(total_iters)) - 1\r\n",
        "\r\n",
        "    # Our update interval should be rounded to an order of magnitude smaller. \r\n",
        "    round_mag = order_of_mag - 1\r\n",
        "\r\n",
        "    # Round down and cast to an int.\r\n",
        "    update_interval = int(round(exact_interval, -round_mag))\r\n",
        "\r\n",
        "    # Don't allow the interval to be zero!\r\n",
        "    if update_interval == 0:\r\n",
        "        update_interval = 1\r\n",
        "\r\n",
        "    return update_interval"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNrbV0c8Wecs"
      },
      "source": [
        "import sys\r\n",
        "if sys.version_info >= (3,):\r\n",
        "    def input(__prompt: any = ...) -> str: ...\r\n",
        "else:\r\n",
        "    def input(__prompt: any = ...) -> any: ...\r\n",
        "    def intern(__string: str) -> str: ..."
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDLJBm3vWegF",
        "outputId": "edee4fc2-0ef9-49d1-c9ef-3f5eccae3de0"
      },
      "source": [
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\r\n",
        "\r\n",
        "\r\n",
        "# Set the seed value all over the place to make this reproducible.\r\n",
        "seed_val = 42\r\n",
        "epoch=2\r\n",
        "import random\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "# Store the average loss after each epoch so we can plot them.\r\n",
        "loss_values = []\r\n",
        "\r\n",
        "# For each epoch...\r\n",
        "for epoch_i in range(0,epoch):\r\n",
        "    \r\n",
        "    # ========================================\r\n",
        "    #               Training\r\n",
        "    # ========================================\r\n",
        "    \r\n",
        "    # Perform one full pass over the training set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    # Measure how long the training epoch takes.\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Reset the total loss for this epoch.\r\n",
        "    total_loss = 0\r\n",
        "\r\n",
        "    # Put the model into training mode. Don't be mislead--the call to \r\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\r\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\r\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # For each batch of training data...\r\n",
        "    for step, batch in enumerate(DL_train):\r\n",
        "\r\n",
        "        # Progress update every 100 batches.\r\n",
        "        if step % 500 == 0 and not step == 0:\r\n",
        "            # Calculate elapsed time in minutes.\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            \r\n",
        "            # Report progress.\r\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(DL_train), elapsed))\r\n",
        "\r\n",
        "        # Unpack this training batch from our dataloader. \r\n",
        "        #\r\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \r\n",
        "        # `to` method.\r\n",
        "        #\r\n",
        "        # `batch` contains three pytorch tensors:\r\n",
        "        #   [0]: input ids \r\n",
        "        #   [1]: attention masks\r\n",
        "        #   [2]: labels \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "\r\n",
        "        # Always clear any previously calculated gradients before performing a\r\n",
        "        # backward pass. PyTorch doesn't do this automatically because \r\n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \r\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\r\n",
        "        model.zero_grad()        \r\n",
        "\r\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\r\n",
        "        # This will return the loss (rather than the model output) because we\r\n",
        "        # have provided the `labels`.\r\n",
        "        # The documentation for this `model` function is here: \r\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                    token_type_ids=None, \r\n",
        "                    attention_mask=b_input_mask, \r\n",
        "                    labels=b_labels)\r\n",
        "        \r\n",
        "        # The call to `model` always returns a tuple, so we need to pull the \r\n",
        "        # loss value out of the tuple.\r\n",
        "        loss = outputs[0]\r\n",
        "\r\n",
        "        # Accumulate the training loss over all of the batches so that we can\r\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "        # single value; the `.item()` function just returns the Python value \r\n",
        "        # from the tensor.\r\n",
        "        total_loss += loss.item()\r\n",
        "\r\n",
        "        # Perform a backward pass to calculate the gradients.\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Clip the norm of the gradients to 1.0.\r\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # Update parameters and take a step using the computed gradient.\r\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\r\n",
        "        # modified based on their gradients, the learning rate, etc.\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Update the learning rate.\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    # Calculate the average loss over the training data.\r\n",
        "    avg_train_loss = total_loss / len(DL_train)            \r\n",
        "    \r\n",
        "    # Store the loss value for plotting the learning curve.\r\n",
        "    loss_values.append(avg_train_loss)\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "      \r\n",
        "print(\"\")\r\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "  Batch   500  of  1,349.    Elapsed: 0:04:12.\n",
            "  Batch 1,000  of  1,349.    Elapsed: 0:08:26.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0:11:24\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "  Batch   500  of  1,349.    Elapsed: 0:04:14.\n",
            "  Batch 1,000  of  1,349.    Elapsed: 0:08:28.\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epoch took: 0:11:25\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "896a9673560344c4d5d26deeceae545902ebac19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsqKn3Da_JI1",
        "outputId": "42b63693-71c0-4413-8479-9a7c83801a7e"
      },
      "source": [
        "#               Validation\n",
        "# ========================================\n",
        "# After the completion of each training epoch, measure our performance on\n",
        "# our validation set.\n",
        "\n",
        "print(\"\")\n",
        "print(\"Running Validation...\")\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# Put the model in evaluation mode--the dropout layers behave differently\n",
        "# during evaluation.\n",
        "model.eval()\n",
        "\n",
        "preds=[]\n",
        "true=[]\n",
        "\n",
        "# Tracking variables \n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "for batch in DL_valid:\n",
        "    \n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and\n",
        "    # speeding up validation\n",
        "    with torch.no_grad():        \n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        # This will return the logits rather than the loss because we have\n",
        "        # not provided labels.\n",
        "        # token_type_ids is the same as the \"segment ids\", which \n",
        "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "    \n",
        "    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "    # values prior to applying an activation function like the softmax.\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    preds.append(logits)\n",
        "    true.append(label_ids)\n",
        "    # Calculate the accuracy for this batch of test sentences.\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    # Accumulate the total accuracy.\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    # Track the number of batches\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.92\n",
            "  Validation took: 0:00:23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4dc41595141dfe7c7cabf671ca0dc1eca444d77d",
        "id": "b67MvVW2_JI6"
      },
      "source": [
        "PATH = \"/content/drive/MyDrive/NER/name.pt\""
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaLmIG2k8A8D"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# Save\r\n",
        "torch.save(model,PATH) \r\n",
        "\r\n",
        "# # Load\r\n",
        "\r\n",
        "# model.load_state_dict(torch.load(PATH))\r\n",
        "# model.eval()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxHG9SotcpwW",
        "outputId": "6373a7bd-03bd-4d79-b11c-efca2f2fe95c"
      },
      "source": [
        "# Load\r\n",
        "import torch\r\n",
        "model1 = torch.load(PATH,map_location='cpu')\r\n",
        "model1.eval()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaForTokenClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_6WCMsTnjyV"
      },
      "source": [
        "test_sentence = \"\"\"\r\n",
        "Mr. Trump’s tweets began just moments after a Fox News report by Mike Tobin, a \r\n",
        "reporter for the network, about protests in Minnesota and elsewhere. \r\n",
        "\"\"\"\r\n",
        "\r\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDWARfR-npNq"
      },
      "source": [
        "tokenized_sentence = tokenizer.encode(test_sentence)\r\n",
        "input_ids = torch.tensor([tokenized_sentence])"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn19IXJyn4M9"
      },
      "source": [
        "\r\n",
        "with torch.no_grad():\r\n",
        "    output = model1(input_ids)\r\n",
        "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl6aExQ2xkge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7365613f-93ab-4105-deeb-ec2d755edb1b"
      },
      "source": [
        "tag_values = list(set(input_data[\"Tag\"].values))\r\n",
        "tag_values.append(\"PAD\")\r\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}\r\n",
        "tag_values"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-art',\n",
              " 'I-nat',\n",
              " 'I-geo',\n",
              " 'I-gpe',\n",
              " 'B-nat',\n",
              " 'B-org',\n",
              " 'B-eve',\n",
              " 'I-per',\n",
              " 'I-eve',\n",
              " 'B-gpe',\n",
              " 'B-per',\n",
              " 'B-tim',\n",
              " 'I-art',\n",
              " 'I-tim',\n",
              " 'I-org',\n",
              " 'B-geo',\n",
              " 'O',\n",
              " 'PAD']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRs_vvlDmHsZ"
      },
      "source": [
        "tag2=['B-art',\r\n",
        " 'B-eve',\r\n",
        " 'B-geo',\r\n",
        " 'B-gpe',\r\n",
        " 'B-nat',\r\n",
        " 'B-org',\r\n",
        " 'B-per',\r\n",
        " 'B-tim',\r\n",
        " 'I-art',\r\n",
        " 'I-eve',\r\n",
        " 'I-geo',\r\n",
        " 'I-gpe',\r\n",
        " 'I-nat',\r\n",
        " 'I-org',\r\n",
        " 'I-per',\r\n",
        " 'I-tim',\r\n",
        " 'O',\r\n",
        " 'PAD']"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-5QSxAlx-vU"
      },
      "source": [
        "tag1=['B-nat',\r\n",
        " 'B-geo',\r\n",
        " 'B-art',\r\n",
        " 'B-tim',\r\n",
        " 'I-gpe',\r\n",
        " 'I-tim',\r\n",
        " 'I-org',\r\n",
        " 'B-eve',\r\n",
        " 'I-nat',\r\n",
        " 'I-eve',\r\n",
        " 'B-org',\r\n",
        " 'I-per',\r\n",
        " 'B-per',\r\n",
        " 'O',\r\n",
        " 'I-art',\r\n",
        " 'I-geo',\r\n",
        " 'B-gpe']"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2D4KAMjDfN3",
        "outputId": "cfe58e92-e3aa-4635-9c5f-8582dc739858"
      },
      "source": [
        "tag2idx = {t: i for i, t in enumerate(tag_values)}\r\n",
        "tag2idx"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-art': 0,\n",
              " 'B-eve': 6,\n",
              " 'B-geo': 15,\n",
              " 'B-gpe': 9,\n",
              " 'B-nat': 4,\n",
              " 'B-org': 5,\n",
              " 'B-per': 10,\n",
              " 'B-tim': 11,\n",
              " 'I-art': 12,\n",
              " 'I-eve': 8,\n",
              " 'I-geo': 2,\n",
              " 'I-gpe': 3,\n",
              " 'I-nat': 1,\n",
              " 'I-org': 14,\n",
              " 'I-per': 7,\n",
              " 'I-tim': 13,\n",
              " 'O': 16,\n",
              " 'PAD': 17}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1WqiZvjn8ea"
      },
      "source": [
        "# join bpe split tokens\r\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\r\n",
        "new_tokens, new_labels = [], []\r\n",
        "for token, label_idx in zip(tokens, label_indices[0]):\r\n",
        "    if token.startswith(\"##\"):\r\n",
        "        new_tokens[-1] = new_tokens[-1] + token[2:]\r\n",
        "    else:\r\n",
        "        new_labels.append(tag_values[label_idx])\r\n",
        "        new_tokens.append(token)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISOdXIE_ukOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87287c3-d8c8-43ae-ec6c-cc828f46d673"
      },
      "source": [
        "for token, label in zip(new_tokens, new_labels):\r\n",
        "    print(\"{}\\t{}\".format(label, token))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I-art\t<s>\n",
            "I-art\t▁Mr\n",
            "B-tim\t.\n",
            "I-tim\t▁Trump\n",
            "I-tim\t’\n",
            "I-tim\ts\n",
            "I-tim\t▁tweet\n",
            "I-tim\ts\n",
            "I-tim\t▁began\n",
            "I-tim\t▁just\n",
            "B-eve\t▁moments\n",
            "B-eve\t▁after\n",
            "I-tim\t▁a\n",
            "I-tim\t▁Fox\n",
            "I-art\t▁News\n",
            "B-tim\t▁report\n",
            "I-tim\t▁by\n",
            "I-tim\t▁Mike\n",
            "I-tim\t▁To\n",
            "I-tim\tbin\n",
            "I-tim\t,\n",
            "I-tim\t▁a\n",
            "I-tim\t▁reporter\n",
            "I-tim\t▁for\n",
            "I-nat\t▁the\n",
            "I-tim\t▁network\n",
            "I-tim\t,\n",
            "I-tim\t▁about\n",
            "I-tim\t▁protest\n",
            "I-tim\ts\n",
            "I-tim\t▁in\n",
            "I-tim\t▁Minnesota\n",
            "I-tim\t▁and\n",
            "I-tim\t▁elsewhere\n",
            "I-tim\t.\n",
            "I-tim\t</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqW_mOfBBkLF"
      },
      "source": [
        "import nltk\r\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbu9sczvybVb"
      },
      "source": [
        "    def generate_ner(sentence):\r\n",
        "        tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\r\n",
        "        model = torch.load(PATH, map_location='cpu')\r\n",
        "        tokenized_sentence = tokenizer.encode(test_sentence)\r\n",
        "        input_ids = torch.tensor([tokenized_sentence])\r\n",
        "        with torch.no_grad():\r\n",
        "            output = model(input_ids)\r\n",
        "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\r\n",
        "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\r\n",
        "        new_tokens, new_labels = [], []\r\n",
        "        for token, label_idx in zip(tokens, label_indices[0]):\r\n",
        "            if token.startswith(\"##\"):\r\n",
        "                new_tokens[-1] = new_tokens[-1] + token[2:]\r\n",
        "            else:\r\n",
        "                new_labels.append(tag1[label_idx])\r\n",
        "                new_tokens.append(token)\r\n",
        "        prediction= [{\"word\":token,\"tag\":label}for token, label in zip(new_tokens, new_labels)]\r\n",
        "        return prediction"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea0qQuJkBNkI",
        "outputId": "ffbec332-b940-473c-8590-8ee90df6ac03"
      },
      "source": [
        "generate_ner(test_sentence)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'tag': 'B-per', 'word': '<s>'},\n",
              " {'tag': 'B-per', 'word': '▁Mr'},\n",
              " {'tag': 'I-per', 'word': '.'},\n",
              " {'tag': 'O', 'word': '▁Trump'},\n",
              " {'tag': 'O', 'word': '’'},\n",
              " {'tag': 'O', 'word': 's'},\n",
              " {'tag': 'O', 'word': '▁tweet'},\n",
              " {'tag': 'O', 'word': 's'},\n",
              " {'tag': 'O', 'word': '▁began'},\n",
              " {'tag': 'O', 'word': '▁just'},\n",
              " {'tag': 'I-org', 'word': '▁moments'},\n",
              " {'tag': 'I-org', 'word': '▁after'},\n",
              " {'tag': 'O', 'word': '▁a'},\n",
              " {'tag': 'O', 'word': '▁Fox'},\n",
              " {'tag': 'B-per', 'word': '▁News'},\n",
              " {'tag': 'I-per', 'word': '▁report'},\n",
              " {'tag': 'O', 'word': '▁by'},\n",
              " {'tag': 'O', 'word': '▁Mike'},\n",
              " {'tag': 'O', 'word': '▁To'},\n",
              " {'tag': 'O', 'word': 'bin'},\n",
              " {'tag': 'O', 'word': ','},\n",
              " {'tag': 'O', 'word': '▁a'},\n",
              " {'tag': 'O', 'word': '▁reporter'},\n",
              " {'tag': 'O', 'word': '▁for'},\n",
              " {'tag': 'B-geo', 'word': '▁the'},\n",
              " {'tag': 'O', 'word': '▁network'},\n",
              " {'tag': 'O', 'word': ','},\n",
              " {'tag': 'O', 'word': '▁about'},\n",
              " {'tag': 'O', 'word': '▁protest'},\n",
              " {'tag': 'O', 'word': 's'},\n",
              " {'tag': 'O', 'word': '▁in'},\n",
              " {'tag': 'O', 'word': '▁Minnesota'},\n",
              " {'tag': 'O', 'word': '▁and'},\n",
              " {'tag': 'O', 'word': '▁elsewhere'},\n",
              " {'tag': 'O', 'word': '.'},\n",
              " {'tag': 'O', 'word': '</s>'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hvn40tECYWR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}